# ğŸš€ DataFlow Platform - Complete Project Documentation

## ğŸ“‹ Project Overview

| Attribute              | Details                              |
| ---------------------- | ------------------------------------ |
| **Project Name**       | DataFlow Platform                    |
| **Domain**             | E-Commerce Data Engineering          |
| **Type**               | End-to-End Data Engineering Platform |
| **Duration**           | 12 months (26 sprints Ã— 2 weeks)     |
| **Total Story Points** | 653                                  |
| **Total PBIs**         | 222                                  |
| **Cost**               | $0 (100% Free & Open Source)         |

### ğŸ¯ Project Goal

Build a production-grade, end-to-end data engineering platform that demonstrates skills required for Big Tech Data Engineering roles. The platform processes e-commerce data through batch and streaming pipelines, implements a modern data lakehouse architecture, and provides analytics dashboards.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATAFLOW PLATFORM ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                    â”‚
â”‚   DATA SOURCES                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚ PostgreSQL  â”‚  â”‚  MongoDB    â”‚  â”‚  REST API   â”‚  â”‚  CSV/JSON   â”‚              â”‚
â”‚   â”‚  (Orders,   â”‚  â”‚ (Products,  â”‚  â”‚ (External   â”‚  â”‚  (Historicalâ”‚              â”‚
â”‚   â”‚  Customers) â”‚  â”‚  Reviews)   â”‚  â”‚   Data)     â”‚  â”‚   Data)     â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚          â”‚                â”‚                â”‚                â”‚                      â”‚
â”‚          â–¼                â–¼                â–¼                â–¼                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚                      INGESTION LAYER                              â”‚            â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚            â”‚
â”‚   â”‚  â”‚ Debezium â”‚  â”‚  Kafka   â”‚  â”‚  Python  â”‚  â”‚  Spark   â”‚         â”‚            â”‚
â”‚   â”‚  â”‚  (CDC)   â”‚  â”‚ Connect  â”‚  â”‚  Scripts â”‚  â”‚  Batch   â”‚         â”‚            â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚           â”‚             â”‚             â”‚             â”‚                              â”‚
â”‚           â–¼             â–¼             â–¼             â–¼                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚                    MESSAGE QUEUE (KAFKA)                          â”‚            â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚            â”‚
â”‚   â”‚  â”‚ orders     â”‚ â”‚ customers  â”‚ â”‚ products   â”‚ â”‚ clickstreamâ”‚    â”‚            â”‚
â”‚   â”‚  â”‚ topic      â”‚ â”‚ topic      â”‚ â”‚ topic      â”‚ â”‚ topic      â”‚    â”‚            â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                               â”‚                                                    â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚           â–¼                   â–¼                   â–¼                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚   â”‚   SPARK     â”‚     â”‚   SPARK     â”‚     â”‚   AIRFLOW   â”‚                         â”‚
â”‚   â”‚  STREAMING  â”‚     â”‚   BATCH     â”‚     â”‚   (DAGs)    â”‚                         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚          â”‚                   â”‚                   â”‚                                 â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                              â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚                 DATA LAKEHOUSE (MinIO + Delta Lake)               â”‚            â”‚
â”‚   â”‚                                                                   â”‚            â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚            â”‚
â”‚   â”‚  â”‚    BRONZE    â”‚  â”‚    SILVER    â”‚  â”‚     GOLD     â”‚           â”‚            â”‚
â”‚   â”‚  â”‚   (Raw Data) â”‚â”€â–¶â”‚  (Cleaned)   â”‚â”€â–¶â”‚  (Business)  â”‚           â”‚            â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                              â”‚                                                     â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚          â–¼                   â–¼                   â–¼                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚   â”‚     dbt     â”‚     â”‚    FEAST    â”‚     â”‚   GREAT     â”‚                         â”‚
â”‚   â”‚   (Trans-   â”‚     â”‚  (Feature   â”‚     â”‚ EXPECTATIONSâ”‚                         â”‚
â”‚   â”‚  formations)â”‚     â”‚   Store)    â”‚     â”‚  (Quality)  â”‚                         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚          â”‚                   â”‚                   â”‚                                 â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                              â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚                      SERVING LAYER                                â”‚            â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚            â”‚
â”‚   â”‚  â”‚  TRINO   â”‚  â”‚ SUPERSET â”‚  â”‚  REDIS   â”‚  â”‚  FastAPI â”‚         â”‚            â”‚
â”‚   â”‚  â”‚ (Query)  â”‚  â”‚  (BI)    â”‚  â”‚ (Cache)  â”‚  â”‚  (API)   â”‚         â”‚            â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                              â”‚                                                     â”‚
â”‚                              â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚                   OBSERVABILITY LAYER                             â”‚            â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚            â”‚
â”‚   â”‚  â”‚PROMETHEUSâ”‚  â”‚ GRAFANA  â”‚  â”‚ DATAHUB  â”‚  â”‚OPENLINEAGâ”‚         â”‚            â”‚
â”‚   â”‚  â”‚(Metrics) â”‚  â”‚(Dashboardâ”‚  â”‚(Catalog) â”‚  â”‚(Lineage) â”‚         â”‚            â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚                   INFRASTRUCTURE LAYER                            â”‚            â”‚
â”‚   â”‚      Docker  â”‚  Kubernetes (minikube)  â”‚  Terraform  â”‚  Git      â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

### **All Technologies (100% Free & Open Source)**

| Category           | Technology            | Purpose                                | Docker Port |
| ------------------ | --------------------- | -------------------------------------- | ----------- |
| **Databases**      | PostgreSQL            | Transactional data (orders, customers) | 5432        |
| **Databases**      | MongoDB               | Document store (products, reviews)     | 27017       |
| **Databases**      | Redis                 | Cache, online feature store            | 6379        |
| **Streaming**      | Apache Kafka          | Message queue, event streaming         | 9092        |
| **Streaming**      | Zookeeper             | Kafka coordination                     | 2181        |
| **Streaming**      | Schema Registry       | Schema management                      | 8081        |
| **CDC**            | Debezium              | Change Data Capture from databases     | 8083        |
| **Processing**     | Apache Spark          | Batch & stream processing              | 8080, 7077  |
| **Storage**        | MinIO                 | S3-compatible object storage           | 9000, 9001  |
| **Table Format**   | Delta Lake            | ACID transactions on data lake         | -           |
| **Orchestration**  | Apache Airflow        | Workflow orchestration                 | 8080        |
| **Transformation** | dbt Core              | SQL transformations                    | -           |
| **Data Quality**   | Great Expectations    | Data validation                        | -           |
| **Data Quality**   | Soda Core             | SQL-based quality checks               | -           |
| **Feature Store**  | Feast                 | ML feature management                  | -           |
| **Query Engine**   | Trino                 | SQL analytics on data lake             | 8080        |
| **Visualization**  | Apache Superset       | BI dashboards                          | 8088        |
| **Monitoring**     | Prometheus            | Metrics collection                     | 9090        |
| **Monitoring**     | Grafana               | Metrics visualization                  | 3000        |
| **Data Catalog**   | DataHub               | Metadata management                    | 9002        |
| **Lineage**        | OpenLineage           | Data lineage tracking                  | -           |
| **Containers**     | Docker                | Containerization                       | -           |
| **Orchestration**  | Kubernetes (minikube) | Container orchestration                | -           |
| **IaC**            | Terraform             | Infrastructure as Code                 | -           |
| **CI/CD**          | GitHub Actions        | Automation                             | -           |

---

## ğŸ“ Project Structure

```
dataflow-platform/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/           # GitHub Actions CI/CD
â”‚   â”‚   â”œâ”€â”€ ci.yml
â”‚   â”‚   â””â”€â”€ cd.yml
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/
â”‚   â””â”€â”€ pull_request_template.md
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ kafka/
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â”‚   â”œâ”€â”€ kafka_consumer.py
â”‚   â”‚   â””â”€â”€ debezium_config.py
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_to_gold.py
â”‚   â”‚   â”‚   â””â”€â”€ cleaning_jobs.py
â”‚   â”‚   â””â”€â”€ streaming/
â”‚   â”‚       â”œâ”€â”€ clickstream_processor.py
â”‚   â”‚       â””â”€â”€ realtime_aggregations.py
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â”œâ”€â”€ expectations/
â”‚   â”‚   â””â”€â”€ soda_checks/
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ helpers.py
â”‚       â””â”€â”€ spark_utils.py
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ bronze_ingestion_dag.py
â”‚   â”œâ”€â”€ silver_transformation_dag.py
â”‚   â”œâ”€â”€ gold_aggregation_dag.py
â”‚   â””â”€â”€ data_quality_dag.py
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_products.sql
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_products.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_date.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ fct_orders.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ daily_sales.sql
â”‚   â”‚   â”‚   â””â”€â”€ customer_lifetime_value.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ macros/
â”‚
â”œâ”€â”€ feature_store/
â”‚   â”œâ”€â”€ feature_store.yaml
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ customer_features.py
â”‚   â”‚   â”œâ”€â”€ product_features.py
â”‚   â”‚   â””â”€â”€ session_features.py
â”‚   â””â”€â”€ materialization/
â”‚
â”œâ”€â”€ data_generators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ customer_generator.py
â”‚   â”œâ”€â”€ order_generator.py
â”‚   â”œâ”€â”€ product_generator.py
â”‚   â””â”€â”€ clickstream_generator.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ setup/
â”‚   â””â”€â”€ runbooks/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ init_databases.sql
â”‚   â”‚   â””â”€â”€ create_topics.sh
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ trino/
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ“Š Data Model

### **E-Commerce Domain Model**

#### **Source Tables (PostgreSQL)**

```sql
-- Customers
CREATE TABLE customers (
    customer_id UUID PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    phone VARCHAR(20),
    address_line1 VARCHAR(255),
    address_line2 VARCHAR(255),
    city VARCHAR(100),
    state VARCHAR(100),
    postal_code VARCHAR(20),
    country VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Products
CREATE TABLE products (
    product_id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100),
    subcategory VARCHAR(100),
    price DECIMAL(10,2),
    cost DECIMAL(10,2),
    sku VARCHAR(50) UNIQUE,
    stock_quantity INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Orders
CREATE TABLE orders (
    order_id UUID PRIMARY KEY,
    customer_id UUID REFERENCES customers(customer_id),
    order_date TIMESTAMP NOT NULL,
    status VARCHAR(50),
    total_amount DECIMAL(12,2),
    shipping_address TEXT,
    payment_method VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Order Items
CREATE TABLE order_items (
    order_item_id UUID PRIMARY KEY,
    order_id UUID REFERENCES orders(order_id),
    product_id UUID REFERENCES products(product_id),
    quantity INTEGER NOT NULL,
    unit_price DECIMAL(10,2),
    discount DECIMAL(10,2) DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Payments
CREATE TABLE payments (
    payment_id UUID PRIMARY KEY,
    order_id UUID REFERENCES orders(order_id),
    amount DECIMAL(12,2),
    payment_method VARCHAR(50),
    payment_status VARCHAR(50),
    transaction_id VARCHAR(255),
    payment_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### **Medallion Architecture Layers**

**Bronze Layer (Raw)**

- `bronze.raw_orders` - CDC events from orders table
- `bronze.raw_customers` - CDC events from customers table
- `bronze.raw_products` - CDC events from products table
- `bronze.raw_clickstream` - Real-time clickstream events
- `bronze.raw_payments` - Payment events

**Silver Layer (Cleaned)**

- `silver.customers` - Deduplicated, cleaned customers
- `silver.products` - Standardized products
- `silver.orders` - Validated orders
- `silver.order_items` - Clean order line items
- `silver.sessions` - Sessionized clickstream

**Gold Layer (Business)**

- `gold.dim_customers` - Customer dimension (SCD Type 2)
- `gold.dim_products` - Product dimension
- `gold.dim_date` - Date dimension
- `gold.fct_orders` - Order facts
- `gold.fct_order_items` - Order item facts
- `gold.daily_sales` - Daily sales aggregates
- `gold.customer_lifetime_value` - CLV calculations
- `gold.product_performance` - Product metrics

---

## ğŸ—“ï¸ Project Timeline

### **Sprint Schedule (26 Sprints)**

| Sprint | Weeks | Epic Focus       | Key Deliverables              |
| ------ | ----- | ---------------- | ----------------------------- |
| 1      | 1-2   | Foundation       | Dev environment, Docker setup |
| 2      | 3-4   | Foundation       | CI/CD pipeline                |
| 3      | 5-6   | Foundation       | Documentation                 |
| 4      | 7-8   | Data Sources     | PostgreSQL, MongoDB           |
| 5      | 9-10  | Data Sources     | Kafka cluster                 |
| 6      | 11-12 | Data Sources     | Debezium CDC                  |
| 7      | 13-14 | Data Lake        | MinIO, Delta Lake             |
| 8      | 15-16 | Data Lake        | Bronze pipelines              |
| 9      | 17-18 | Data Lake        | Schema management             |
| 10     | 19-20 | Batch Processing | Spark cluster                 |
| 11     | 21-22 | Batch Processing | Silver layer                  |
| 12     | 23-24 | Batch Processing | SCD Type 2                    |
| 13     | 25-26 | Orchestration    | Airflow setup                 |
| 14     | 27-28 | Orchestration    | DAGs & alerting               |
| 15     | 29-30 | Transformation   | dbt setup, staging            |
| 16     | 31-32 | Transformation   | Fact models                   |
| 17     | 33-34 | Transformation   | Business marts                |
| 18     | 35-36 | Data Quality     | Great Expectations            |
| 19     | 37-38 | Data Quality     | Quality automation            |
| 20     | 39-40 | Streaming        | Spark Streaming               |
| 21     | 41-42 | Streaming        | Aggregations                  |
| 22     | 43-44 | Feature Store    | Feast setup                   |
| 23     | 45-46 | Feature Store    | Online serving                |
| 24     | 47-48 | Analytics        | Trino, Superset               |
| 25     | 49-50 | Analytics        | Dashboards                    |
| 26     | 51-52 | Observability    | Monitoring, catalog           |

---

## ğŸ“‹ Epic & Feature Breakdown

### **EPIC-1: Project Foundation & Infrastructure** (Sprints 1-3, 34 pts)

- FEAT-1.1: Development Environment Setup
- FEAT-1.2: Docker Infrastructure
- FEAT-1.3: CI/CD Pipeline Setup
- FEAT-1.4: Project Documentation Structure

### **EPIC-2: Data Sources & Ingestion** (Sprints 4-6, 42 pts)

- FEAT-2.1: PostgreSQL Database Setup
- FEAT-2.2: MongoDB Setup
- FEAT-2.3: Apache Kafka Cluster
- FEAT-2.4: Debezium CDC Implementation

### **EPIC-3: Data Lake & Bronze Layer** (Sprints 7-9, 38 pts)

- FEAT-3.1: MinIO Object Storage
- FEAT-3.2: Delta Lake Configuration
- FEAT-3.3: Bronze Layer Pipelines
- FEAT-3.4: Schema Evolution & Management

### **EPIC-4: Batch Processing & Silver Layer** (Sprints 10-12, 44 pts)

- FEAT-4.1: Spark Cluster Setup
- FEAT-4.2: Data Cleaning Jobs
- FEAT-4.3: Silver Layer Implementation
- FEAT-4.4: SCD Type 2 & Historical Tracking

### **EPIC-5: Orchestration & Scheduling** (Sprints 13-14, 32 pts)

- FEAT-5.1: Apache Airflow Installation
- FEAT-5.2: Pipeline DAGs Development
- FEAT-5.3: Monitoring & Alerting

### **EPIC-6: Transformation & Gold Layer** (Sprints 15-17, 46 pts)

- FEAT-6.1: dbt Project Setup
- FEAT-6.2: Dimension Models
- FEAT-6.3: Fact Models
- FEAT-6.4: Business Marts

### **EPIC-7: Data Quality & Testing** (Sprints 18-19, 30 pts)

- FEAT-7.1: Great Expectations Setup
- FEAT-7.2: Soda Core Integration
- FEAT-7.3: Quality Automation

### **EPIC-8: Stream Processing** (Sprints 20-21, 34 pts)

- FEAT-8.1: Spark Streaming Setup
- FEAT-8.2: Real-time Pipelines
- FEAT-8.3: Streaming Aggregations

### **EPIC-9: Feature Store** (Sprints 22-23, 32 pts)

- FEAT-9.1: Feast Installation
- FEAT-9.2: Feature Engineering
- FEAT-9.3: Online Feature Serving

### **EPIC-10: Analytics & Visualization** (Sprints 24-25, 30 pts)

- FEAT-10.1: Trino Query Engine
- FEAT-10.2: Apache Superset Dashboards
- FEAT-10.3: Self-Service Analytics

### **EPIC-11: Observability & Governance** (Sprint 26, 26 pts)

- FEAT-11.1: Prometheus & Grafana Monitoring
- FEAT-11.2: DataHub Data Catalog
- FEAT-11.3: OpenLineage Data Lineage

### **EPIC-12: Documentation & Portfolio** (Ongoing, 20 pts)

- FEAT-12.1: Technical Documentation
- FEAT-12.2: Portfolio & Career Materials

---

## ğŸ³ Docker Services

### **docker-compose.yml Services**

```yaml
services:
  # Databases
  postgres: # Port 5432 - Transactional DB
  mongodb: # Port 27017 - Document DB
  redis: # Port 6379 - Cache/Online Store

  # Streaming
  zookeeper: # Port 2181 - Kafka coordination
  kafka: # Port 9092 - Message broker
  schema-registry: # Port 8081 - Schema management
  kafka-connect: # Port 8083 - Connectors (Debezium)
  kafdrop: # Port 9000 - Kafka UI

  # Processing
  spark-master: # Port 8080, 7077 - Spark master
  spark-worker: # Spark worker nodes

  # Storage
  minio: # Port 9000, 9001 - Object storage

  # Orchestration
  airflow-webserver: # Port 8080 - Airflow UI
  airflow-scheduler: # Airflow scheduler
  airflow-worker: # Airflow workers

  # Analytics
  trino: # Port 8080 - Query engine
  superset: # Port 8088 - BI dashboards

  # Monitoring
  prometheus: # Port 9090 - Metrics
  grafana: # Port 3000 - Dashboards

  # Governance
  datahub-gms: # DataHub backend
  datahub-frontend: # Port 9002 - DataHub UI
```

---

## ğŸ”‘ Environment Variables

```env
# Project
PROJECT_NAME=dataflow-platform
ENVIRONMENT=development

# PostgreSQL
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=ecommerce
POSTGRES_USER=dataflow
POSTGRES_PASSWORD=your_password

# MongoDB
MONGODB_HOST=mongodb
MONGODB_PORT=27017
MONGODB_DB=products
MONGODB_USER=dataflow
MONGODB_PASSWORD=your_password

# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_SCHEMA_REGISTRY=http://schema-registry:8081

# MinIO
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=your_secret_key
MINIO_BUCKET_BRONZE=bronze
MINIO_BUCKET_SILVER=silver
MINIO_BUCKET_GOLD=gold

# Spark
SPARK_MASTER=spark://spark-master:7077

# Airflow
AIRFLOW_UID=50000
AIRFLOW__CORE__EXECUTOR=LocalExecutor

# Redis
REDIS_HOST=redis
REDIS_PORT=6379

# Trino
TRINO_HOST=trino
TRINO_PORT=8080
```

---

## ğŸ“Š Key Metrics & KPIs

### **Data Pipeline Metrics**

- Records processed per minute
- Pipeline latency (source to gold)
- Data freshness (time since last update)
- Error rate per pipeline
- Kafka consumer lag

### **Data Quality Metrics**

- Test pass rate (%)
- Data completeness (%)
- Schema violations count
- Anomaly detection alerts

### **Business Metrics (Dashboards)**

- Daily/Weekly/Monthly Revenue
- Order Count & Average Order Value
- Customer Acquisition & Retention
- Product Performance
- Conversion Funnel Metrics

---

## ğŸš€ Common Commands

```bash
# Start all services
make up
docker compose up -d

# Stop all services
make down
docker compose down

# View logs
make logs
docker compose logs -f [service_name]

# Run tests
make test
pytest tests/ -v

# Run linting
make lint
flake8 src/ tests/

# Format code
make format
black src/ tests/
isort src/ tests/

# Run dbt
cd dbt && dbt run
cd dbt && dbt test

# Airflow CLI
docker compose exec airflow-webserver airflow dags list
docker compose exec airflow-webserver airflow tasks test [dag_id] [task_id] [date]

# Kafka
docker compose exec kafka kafka-topics --list --bootstrap-server localhost:9092
docker compose exec kafka kafka-console-consumer --topic [topic_name] --bootstrap-server localhost:9092

# Spark submit
docker compose exec spark-master spark-submit --master spark://spark-master:7077 /app/jobs/my_job.py
```

---

## ğŸ”§ Troubleshooting

### **Common Issues**

| Issue                    | Solution                                                |
| ------------------------ | ------------------------------------------------------- |
| Docker out of memory     | Increase Docker RAM to 8GB+ in settings                 |
| Kafka not starting       | Check Zookeeper is healthy first                        |
| Spark job fails          | Check executor memory settings                          |
| Airflow DAG not showing  | Run `airflow dags list` and check for import errors     |
| MinIO connection refused | Verify MinIO container is running and ports are exposed |
| dbt connection error     | Check profiles.yml configuration                        |

### **Useful Debug Commands**

```bash
# Check container health
docker compose ps

# View container logs
docker compose logs [service_name] --tail 100

# Enter container shell
docker compose exec [service_name] bash

# Check network connectivity
docker compose exec [service_name] ping [other_service]

# Restart specific service
docker compose restart [service_name]
```

---

## ğŸ“š Learning Resources

### **Courses (Free)**

- DataTalks.Club Data Engineering Bootcamp
- dbt Learn (dbt Fundamentals)
- Confluent Developer (Kafka)
- Astronomer Academy (Airflow)

### **Documentation**

- Apache Spark: https://spark.apache.org/docs/latest/
- Apache Kafka: https://kafka.apache.org/documentation/
- Apache Airflow: https://airflow.apache.org/docs/
- dbt: https://docs.getdbt.com/
- Delta Lake: https://docs.delta.io/
- Great Expectations: https://docs.greatexpectations.io/
- Feast: https://docs.feast.dev/

---

## ğŸ¯ Resume Bullets (After Completion)

```
â€¢ Architected end-to-end data platform processing 1M+ daily events using Kafka,
  Spark, and Delta Lake, implementing medallion architecture with 99.9% data accuracy

â€¢ Built real-time CDC pipeline with Debezium capturing database changes with
  sub-minute latency, enabling near real-time analytics

â€¢ Designed and implemented ML feature store using Feast, serving 100+ features
  with <10ms p99 latency for online inference

â€¢ Created comprehensive data quality framework using Great Expectations,
  implementing 200+ automated checks reducing data incidents by 70%

â€¢ Developed self-service analytics platform with Apache Superset, enabling
  business users to create custom dashboards and reports

â€¢ Implemented data observability solution with Prometheus, Grafana, and DataHub,
  providing end-to-end lineage tracking and monitoring
```

---

## ğŸ†˜ Getting Help

When asking AI for help with this project, provide:

1. **Which PBI/Feature you're working on**: e.g., "I'm working on PBI-47: Deploy Debezium connector"
2. **Current Sprint**: e.g., "Sprint 6"
3. **Technology involved**: e.g., "Debezium, Kafka Connect, PostgreSQL"
4. **Error message or issue**: Copy exact error
5. **What you've tried**: List attempted solutions

### **Example Help Request**

```
I'm working on PBI-47 (Deploy and test Debezium connector) in Sprint 6.

I'm trying to set up Debezium PostgreSQL connector but getting this error:
[paste error message]

My docker-compose has Kafka Connect running. I've verified:
- PostgreSQL is running
- Kafka is running
- wal_level is set to 'logical'

What could be wrong?
```

---

## ğŸ“„ License

MIT License - See LICENSE file for details.

---

**Last Updated**: February 2026
**Project Duration**: 12 months
**Total Effort**: 222 PBIs, 653 Story Points

